{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1bdf5dcd9f3048f9ad39ffc0e5037334": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b09b21be06de43b088679d472e5874ba",
              "IPY_MODEL_5925203a2b3c4c64831be1354a093134",
              "IPY_MODEL_50dd9e47133d4e4c8009f212b74f65b9"
            ],
            "layout": "IPY_MODEL_addd956e47e84a72822e8f2931092a1e"
          }
        },
        "b09b21be06de43b088679d472e5874ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76f9e796e67546aa8bc3fc4dbd5826e4",
            "placeholder": "​",
            "style": "IPY_MODEL_021b424dd606405f812d650f0ee64a89",
            "value": "epoch:1 loss:7.082641887903749:  38%"
          }
        },
        "5925203a2b3c4c64831be1354a093134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41d06833f2554c7c90154492a552a36c",
            "max": 3592,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0faa92443b5a41a4b05f3ff9de93ca6b",
            "value": 1372
          }
        },
        "50dd9e47133d4e4c8009f212b74f65b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f912a336c8a3489dae9eec1b1d0014e5",
            "placeholder": "​",
            "style": "IPY_MODEL_cdaa4e6616404e8d8916c16741b88074",
            "value": " 1372/3592 [00:19&lt;00:30, 72.98it/s]"
          }
        },
        "addd956e47e84a72822e8f2931092a1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76f9e796e67546aa8bc3fc4dbd5826e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "021b424dd606405f812d650f0ee64a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41d06833f2554c7c90154492a552a36c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0faa92443b5a41a4b05f3ff9de93ca6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f912a336c8a3489dae9eec1b1d0014e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdaa4e6616404e8d8916c16741b88074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zihvvan/DeepLearning/blob/main/zihvvan/ch09_DL_13_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 불러오기"
      ],
      "metadata": {
        "id": "Ww4ZmwhFAO7I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8mUWwkM-7eSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f541983c-803e-46b6-f62a-be33496961c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-07 06:08:41--  https://github.com/BigData23th/Data/raw/main/corpus.txt\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/BigData23th/Data/main/corpus.txt [following]\n",
            "--2023-04-07 06:08:41--  https://raw.githubusercontent.com/BigData23th/Data/main/corpus.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 253511 (248K) [text/plain]\n",
            "Saving to: ‘corpus.txt’\n",
            "\n",
            "corpus.txt          100%[===================>] 247.57K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-04-07 06:08:41 (7.91 MB/s) - ‘corpus.txt’ saved [253511/253511]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Tab-delimited Bilingual Sentence Pairs\n",
        "# 출처 : http://www.manythings.org/anki\n",
        "!wget https://github.com/BigData23th/Data/raw/main/corpus.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 파일을 전처리\n",
        "\n",
        "import string # punctuation\n",
        "\n",
        "l = [] # 특수문자를 지운 문장들을 받아줄 리스트\n",
        "\n",
        "# with -> torch.no_grad()? => 특정한 객체가 생성이 되었을 때 with 구문이 끝나면 close 반환\n",
        "with open(\"./corpus.txt\", 'r', encoding='utf-8') as f:\n",
        "    # open(경로, 'r', encoding=인코딩방식): 파일을 읽어와줌 (텍스트파일)\n",
        "    # with .... -> 특정한 객체를 생성시키고, with 구문이 끝나면 해당 객체를 삭제 (반환)\n",
        "    # open () as f -> open을 통해 읽어들여온 파일을 f라는 이름에 변수에 할당\n",
        "    lines = f.read().split('\\n') # '\\n' = 엔터 = 개행문자\n",
        "    # 파일을 읽어온 다음에, 엔터(줄) 기준으로 쪼개줘라 -> 문장별로 리스트화\n",
        "    # lines = ['...', '...', '문장...']\n",
        "    for line in lines: # 문장\n",
        "        # 특수문자를 지우고 모든 글자를 소문자로 변경\n",
        "        txt = \"\".join(v for v in line if not v in string.punctuation).lower()\n",
        "        l.append(txt)"
      ],
      "metadata": {
        "id": "a-aQlWAjAo6A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string # punctuation\n",
        "\n",
        "# l = []\n",
        "\n",
        "with open('./corpus.txt', 'r', encoding='utf-8') as f:\n",
        "    # print(f) # 불러온 파일 객체\n",
        "    # print(f.read()) # 파일 객체 -> 통으로 된 문자열\n",
        "    # print(f.read().split('\\n')) # 문자열 -> 엔터(\\n) 기준으로 분할해서 리스트화\n",
        "    # lines = f.read().split('\\n')\n",
        "    # for line in lines: # 반복문\n",
        "    #     txt = \"\".join(v for v in line if not v in string.punctuation).lower()\n",
        "    #     l.append(txt)\n",
        "    l = [\"\".join(v for v in line if not v in string.punctuation).lower()\n",
        "            for line in f.read().split('\\n')]\n",
        "l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpEobNa4eGZU",
        "outputId": "f70467ab-f50e-475f-e264-39bc71e67acd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['go\\t가',\n",
              " 'hi\\t안녕',\n",
              " 'run\\t뛰어',\n",
              " 'run\\t뛰어',\n",
              " 'who\\t누구',\n",
              " 'wow\\t우와',\n",
              " 'fire\\t쏴',\n",
              " 'help\\t도와줘',\n",
              " 'jump\\t점프',\n",
              " 'jump\\t점프해',\n",
              " 'wait\\t기다려',\n",
              " 'wait\\t잠깐',\n",
              " 'wait\\t기다려',\n",
              " 'begin\\t시작해',\n",
              " 'hello\\t안녕',\n",
              " 'i see\\t알았어',\n",
              " 'i try\\t시도해볼게',\n",
              " 'i won\\t내가 이겼어',\n",
              " 'oh no\\t아니 이런',\n",
              " 'relax\\t진정해',\n",
              " 'shoot\\t쏴',\n",
              " 'smile\\t웃어',\n",
              " 'attack\\t공격',\n",
              " 'attack\\t공격해',\n",
              " 'freeze\\t꼼짝마',\n",
              " 'get up\\t일어나',\n",
              " 'got it\\t알겠어',\n",
              " 'hug me\\t안아줘',\n",
              " 'i know\\t알아',\n",
              " 'i work\\t나 일해',\n",
              " 'listen\\t들어',\n",
              " 'no way\\t절대 아니야',\n",
              " 'no way\\t그럴리가',\n",
              " 'thanks\\t고마워',\n",
              " 'we try\\t우리는 시도할거야',\n",
              " 'we won\\t우리가 이겼어',\n",
              " 'why me\\t왜 나야',\n",
              " 'awesome\\t굉장해',\n",
              " 'be fair\\t공정하게 해',\n",
              " 'beat it\\t저리 가',\n",
              " 'call us\\t우리한테 연락해',\n",
              " 'come in\\t들어와',\n",
              " 'come on\\t어서',\n",
              " 'get out\\t나가',\n",
              " 'go away\\t저리 가',\n",
              " 'go away\\t저리 가',\n",
              " 'goodbye\\t안녕',\n",
              " 'he came\\t그가 왔어',\n",
              " 'he came\\t그 사람이 왔어',\n",
              " 'help me\\t도와줘',\n",
              " 'help me\\t도와줘',\n",
              " 'hit tom\\t톰을 때려',\n",
              " 'i agree\\t동의해',\n",
              " 'im sad\\t슬퍼',\n",
              " 'me too\\t나도',\n",
              " 'open up\\t열어',\n",
              " 'perfect\\t완벽해',\n",
              " 'show me\\t보여줘',\n",
              " 'shut up\\t시끄러워',\n",
              " 'skip it\\t건너뛰어',\n",
              " 'stop it\\t그만해',\n",
              " 'tell me\\t말해',\n",
              " 'tom won\\t톰이 이겼어',\n",
              " 'wake up\\t일어나',\n",
              " 'wash up\\t설거지 해',\n",
              " 'welcome\\t어서오세요',\n",
              " 'welcome\\t환영합니다',\n",
              " 'who won\\t누가 이겼어',\n",
              " 'why not\\t왜 안돼',\n",
              " 'cheer up\\t힘내',\n",
              " 'cool off\\t진정해',\n",
              " 'get lost\\t꺼져',\n",
              " 'go ahead\\t계속해',\n",
              " 'good job\\t잘했어',\n",
              " 'grab tom\\t톰을 잡아',\n",
              " 'how cute\\t귀엽잖아',\n",
              " 'how cute\\t이렇게 귀엽다니',\n",
              " 'how deep\\t얼마나 깊게',\n",
              " 'hurry up\\t서둘러',\n",
              " 'i forgot\\t잊어버렸어',\n",
              " 'im ugly\\t나는 못 생겼다',\n",
              " 'it hurts\\t아파',\n",
              " 'it works\\t동작하네',\n",
              " 'it works\\t작동하네',\n",
              " 'it works\\t되네',\n",
              " 'lets go\\t가자',\n",
              " 'look out\\t조심해',\n",
              " 'sit down\\t앉아',\n",
              " 'sit here\\t여기 앉아',\n",
              " 'speak up\\t크게 말해',\n",
              " 'stand up\\t일어서',\n",
              " 'tell tom\\t톰한테 말해',\n",
              " 'tell tom\\t톰에게 말해',\n",
              " 'they won\\t그들이 이겼어',\n",
              " 'they won\\t그 사람들이 이겼어',\n",
              " 'tom died\\t톰이 죽었어',\n",
              " 'tom left\\t톰이 떠났어',\n",
              " 'tom left\\t톰은 떠났어',\n",
              " 'tom lied\\t톰이 거짓말을 했어',\n",
              " 'tom lied\\t톰이 거짓말했어',\n",
              " 'tom lost\\t톰이 졌어',\n",
              " 'tom paid\\t톰이 지불했어',\n",
              " 'tom quit\\t톰이 그만둬',\n",
              " 'tom wept\\t톰이 눈물을 흘렸어',\n",
              " 'too late\\t너무 늦어',\n",
              " 'trust me\\t날 믿어',\n",
              " 'try hard\\t열심히 해',\n",
              " 'try some\\t좀 먹어봐',\n",
              " 'try this\\t이거 시도해봐',\n",
              " 'what for\\t뭐 하러',\n",
              " 'what fun\\t재밌잖아',\n",
              " 'what fun\\t이렇게 재미있을 수가',\n",
              " 'who died\\t누가 죽었어',\n",
              " 'who quit\\t누가 그만둬',\n",
              " 'answer me\\t대답해',\n",
              " 'birds fly\\t새가 날고 있네',\n",
              " 'calm down\\t진정해',\n",
              " 'come here\\t여기로 와',\n",
              " 'come home\\t집에 와',\n",
              " 'dogs bark\\t개가 짖네',\n",
              " 'dont lie\\t거짓말 하지 마',\n",
              " 'dont lie\\t거짓말 하지 마세요',\n",
              " 'fantastic\\t끝내주네',\n",
              " 'follow me\\t따라와',\n",
              " 'forget it\\t잊어버려',\n",
              " 'forget me\\t날 잊어',\n",
              " 'forget me\\t날 잊어버려',\n",
              " 'get ready\\t준비해',\n",
              " 'good luck\\t행운을 빌어',\n",
              " 'good luck\\t잘 되길 바라',\n",
              " 'grab this\\t저걸 움켜쥐어',\n",
              " 'hands off\\t손대지 마',\n",
              " 'he smiled\\t그 사람은 미소지었어',\n",
              " 'hold this\\t저걸 잡고 있어',\n",
              " 'how awful\\t끔찍해라',\n",
              " 'how awful\\t이렇게 끔찍할 수가',\n",
              " 'i fainted\\t나는 기절했어',\n",
              " 'i laughed\\t나는 웃었어',\n",
              " 'i promise\\t약속할게',\n",
              " 'im sorry\\t미안해',\n",
              " 'im sorry\\t미안해요',\n",
              " 'im sorry\\t죄송합니다',\n",
              " 'im sorry\\t유감입니다',\n",
              " 'it rained\\t비가 왔어',\n",
              " 'it rained\\t비가 내렸어',\n",
              " 'it snowed\\t눈이 왔어',\n",
              " 'it stinks\\t냄새나',\n",
              " 'its 745\\t지금 7시 45분이야',\n",
              " 'kill them\\t그들을 죽여라',\n",
              " 'leave now\\t당장 떠나',\n",
              " 'of course\\t물론이지',\n",
              " 'of course\\t물론이죠',\n",
              " 'oh please\\t아 제발',\n",
              " 'read this\\t이걸 읽어',\n",
              " 'say hello\\t인사해',\n",
              " 'see below\\t아래를 봐',\n",
              " 'seriously\\t진심이야',\n",
              " 'sit there\\t거기 앉아',\n",
              " 'sit tight\\t잠자코 있어',\n",
              " 'start now\\t당장 시작해',\n",
              " 'stay calm\\t침착해',\n",
              " 'stay here\\t여기에 있어',\n",
              " 'step back\\t물러서',\n",
              " 'stop here\\t여기서 멈춰',\n",
              " 'take care\\t주의하세요',\n",
              " 'take this\\t가져',\n",
              " 'take this\\t이거 가져',\n",
              " 'take this\\t이걸 가져',\n",
              " 'thank you\\t고마워',\n",
              " 'then what\\t그래서',\n",
              " 'they left\\t그들이 떠났어',\n",
              " 'they left\\t그 사람들은 떠났어',\n",
              " 'they left\\t그들은 떠났어',\n",
              " 'they lied\\t그들이 거짓말 쳤어',\n",
              " 'they lost\\t그들이 졌어',\n",
              " 'they lost\\t그 사람들이 졌어',\n",
              " 'tom cried\\t톰은 울었어',\n",
              " 'tom dozed\\t톰이 꾸벅 졸았어',\n",
              " 'tom drove\\t톰이 운전했어',\n",
              " 'tom knits\\t톰이 뜨개질 하고 있어',\n",
              " 'tom knows\\t톰은 알고 있어',\n",
              " 'try again\\t다시 한 번 해봐',\n",
              " 'turn left\\t왼쪽으로 돌아',\n",
              " 'turn left\\t좌회전해',\n",
              " 'wait here\\t여기서 기다려',\n",
              " 'watch out\\t조심해',\n",
              " 'we talked\\t우린 서로 얘기했어',\n",
              " 'we waited\\t우린 기다렸어',\n",
              " 'well done\\t잘 했어',\n",
              " 'who cares\\t누가 신경써',\n",
              " 'who knows\\t누가 알아',\n",
              " 'wonderful\\t멋져',\n",
              " 'you idiot\\t이 바보야',\n",
              " 'you tried\\t가상해',\n",
              " 'all aboard\\t모두 타',\n",
              " 'ask anyone\\t누군가에게 물어봐',\n",
              " 'be careful\\t조심해',\n",
              " 'be patient\\t참아',\n",
              " 'be patient\\t인내심을 가져',\n",
              " 'birds sing\\t새가 노래하네',\n",
              " 'bring wine\\t와인 가져와',\n",
              " 'bring wine\\t와인을 가져와',\n",
              " 'carry this\\t이거 날라',\n",
              " 'carry this\\t이거 운반해',\n",
              " 'check this\\t이거 확인해봐',\n",
              " 'choose one\\t하나 골라',\n",
              " 'come again\\t다시 와',\n",
              " 'come alone\\t혼자서 와',\n",
              " 'come along\\t따라와',\n",
              " 'come quick\\t빨리 와',\n",
              " 'definitely\\t절대로',\n",
              " 'dont talk\\t말하지 마',\n",
              " 'eat slowly\\t천천히 먹어',\n",
              " 'face facts\\t진실을 마주해',\n",
              " 'fire burns\\t불타네',\n",
              " 'follow him\\t저 사람을 따라가',\n",
              " 'forget tom\\t톰은 잊어버려',\n",
              " 'forget him\\t그 사람에 대해선 잊어 버려',\n",
              " 'forgive me\\t날 용서해',\n",
              " 'forgive us\\t우릴 용서해 줘',\n",
              " 'forgive us\\t우리를 용서해 줘',\n",
              " 'god exists\\t신은 존재해',\n",
              " 'he is nice\\t걔 괜찮아',\n",
              " 'he is tall\\t그 사람은 키가 커',\n",
              " 'hey relax\\t이봐 진정해',\n",
              " 'hold still\\t가만히 있으세요',\n",
              " 'hold still\\t가만히 있어',\n",
              " 'how lovely\\t어찌나 사랑스러운지',\n",
              " 'how tragic\\t너무 슬퍼',\n",
              " 'how tragic\\t이렇게나 슬프다니',\n",
              " 'hows work\\t일은 잘 되니',\n",
              " 'hows work\\t일은 어때',\n",
              " 'hurry back\\t빨리 와',\n",
              " 'i am human\\t나는 인간이야',\n",
              " 'i chuckled\\t난 킬킬 웃었어',\n",
              " 'i disagree\\t난 반대야',\n",
              " 'i envy him\\t그 사람이 부러워',\n",
              " 'i envy you\\t네가 부러워',\n",
              " 'i felt bad\\t난 기분이 나빴다',\n",
              " 'i remember\\t기억하고 있어',\n",
              " 'i want tom\\t난 톰을 원해',\n",
              " 'ignore tom\\t톰은 무시해',\n",
              " 'ignore him\\t그 사람은 무시해',\n",
              " 'is tom ill\\t톰은 아파',\n",
              " 'is that ok\\t괜찮은 거예요',\n",
              " 'it is warm\\t따뜻해',\n",
              " 'keep going\\t계속 가고 있어봐',\n",
              " 'keep quiet\\t조용히 해',\n",
              " 'keep still\\t계속 조용히 하고 있어봐',\n",
              " 'let me die\\t나 좀 죽게 내버려둬',\n",
              " 'look there\\t저기를 봐',\n",
              " 'love hurts\\t사랑은 아프다',\n",
              " 'mama cried\\t엄마가 울었어',\n",
              " 'mama cried\\t엄마는 울었어',\n",
              " 'never mind\\t신경쓰지마',\n",
              " 'no comment\\t할 말이 없어',\n",
              " 'no problem\\t문제 없어',\n",
              " 'once again\\t한 번 더',\n",
              " 'please sit\\t앉아 줘',\n",
              " 'please sit\\t제발 앉아',\n",
              " 'quiet down\\t조용히 해',\n",
              " 'sing along\\t따라 불러',\n",
              " 'start here\\t여기서 시작해',\n",
              " 'start over\\t다시 해',\n",
              " 'step aside\\t옆으로 비켜',\n",
              " 'stop lying\\t거짓말 그만 쳐',\n",
              " 'study hard\\t열심히 공부해',\n",
              " 'that hurts\\t아파',\n",
              " 'that hurts\\t그거 아프네',\n",
              " 'tom agreed\\t톰이 동의했어',\n",
              " 'tom cheats\\t톰이 사기 쳐',\n",
              " 'tom danced\\t톰은 춤을 췄어',\n",
              " 'tom drives\\t톰은 운전할 수 있어',\n",
              " 'tom failed\\t톰이 실패했어',\n",
              " 'tom forgot\\t톰이 잊었어',\n",
              " 'tom fought\\t톰이 싸웠어',\n",
              " 'tom gasped\\t톰이 헉 소리를 냈어',\n",
              " 'tom helped\\t톰이 도와줬어',\n",
              " 'tom jumped\\t톰이 점프했어',\n",
              " 'tom looked\\t톰이 쳐다봤어',\n",
              " 'tom moaned\\t톰이 끙끙댔어',\n",
              " 'tom nodded\\t톰이 고개를 끄덕였어',\n",
              " 'tom sighed\\t톰은 한숨 쉬었어',\n",
              " 'tom smiled\\t톰이 웃었어',\n",
              " 'tom snores\\t톰은 코를 골아',\n",
              " 'tom waited\\t톰은 기다렸어',\n",
              " 'tom yawned\\t톰이 하품했어',\n",
              " 'turn right\\t오른쪽으로 돌아',\n",
              " 'turn right\\t우회전해',\n",
              " 'you called\\t불렀어',\n",
              " 'you called\\t불렀니',\n",
              " 'you decide\\t네가 정해',\n",
              " 'anyone home\\t누구 집에 있어',\n",
              " 'anyone hurt\\t누군가 다쳤어',\n",
              " 'be cheerful\\t활기차게 해',\n",
              " 'be yourself\\t너 자신이 돼',\n",
              " 'be yourself\\t너답게 있어',\n",
              " 'boys do cry\\t남자애도 운다',\n",
              " 'check again\\t다시 확인해',\n",
              " 'come aboard\\t외국으로 와',\n",
              " 'come closer\\t가까이 와',\n",
              " 'come inside\\t안으로 들어와',\n",
              " 'finish this\\t이걸 끝내',\n",
              " 'get serious\\t진지하게 해',\n",
              " 'he chuckled\\t그 사람 킬킬 웃었어',\n",
              " 'he resigned\\t그 사람은 은퇴했어',\n",
              " 'how curious\\t정말 흥미로운데',\n",
              " 'how strange\\t참 이상하네',\n",
              " 'i dont lie\\t나는 거짓말 하지 않습니다',\n",
              " 'i dont lie\\t난 거짓말 안해',\n",
              " 'i dont lie\\t나는 거짓말 하지 않아',\n",
              " 'i exercised\\t난 운동했어',\n",
              " 'i overslept\\t나는 늦잠잤어',\n",
              " 'im nervous\\t긴장돼요',\n",
              " 'im nervous\\t떨려요',\n",
              " 'im shocked\\t충격이야',\n",
              " 'ignore that\\t그건 무시해',\n",
              " 'its a pity\\t안타까워요',\n",
              " 'its unfair\\t이건 불공평해',\n",
              " 'keep moving\\t계속 움직이고 있어봐',\n",
              " 'keep trying\\t계속 시도해',\n",
              " 'look around\\t둘러봐',\n",
              " 'money talks\\t돈이 최고야',\n",
              " 'my dog died\\t내 개는 죽었어',\n",
              " 'nice timing\\t타이밍 좋네',\n",
              " 'nice timing\\t좋은 타이밍이야',\n",
              " 'nobody came\\t아무도 안왔어',\n",
              " 'nobody died\\t아무도 안 죽었어',\n",
              " 'nobody died\\t아무도 죽지 않았어',\n",
              " 'nobody lied\\t아무도 거짓말을 안 했어',\n",
              " 'nobody lied\\t아무도 거짓말을 하지 않았어',\n",
              " 'plants grow\\t식물이 자란다',\n",
              " 'please sing\\t노래 부탁해',\n",
              " 'please stay\\t제발 남아 있어줘',\n",
              " 'please stop\\t제발 멈춰',\n",
              " 'release him\\t그 사람을 놓아 줘',\n",
              " 'release him\\t그를 놓아 줘',\n",
              " 'remember it\\t기억해',\n",
              " 'say goodbye\\t작별인사해',\n",
              " 'say nothing\\t아무 말도 하지 마',\n",
              " 'stop crying\\t그만 울어',\n",
              " 'stop moving\\t움직이지 마',\n",
              " 'they danced\\t그들은 춤췄어',\n",
              " 'they danced\\t그 사람들은 춤을 췄어',\n",
              " 'they hugged\\t그 사람들은 서로 포옹했어',\n",
              " 'they kissed\\t그 사람들 서로 키스했어',\n",
              " 'they obeyed\\t그 사람들은 복종했어',\n",
              " 'they smiled\\t그들이 웃었어',\n",
              " 'they smiled\\t그 사람들이 웃었어',\n",
              " 'tom blushed\\t톰의 얼굴이 빨개졌어',\n",
              " 'tom cheated\\t톰이 사기 쳤어',\n",
              " 'tom cheated\\t톰이 속임수를 썼어',\n",
              " 'tom clapped\\t톰이 박수쳤어',\n",
              " 'tom clapped\\t톰이 박수를 쳤어',\n",
              " 'tom coughed\\t톰은 기침했어',\n",
              " 'tom drowned\\t톰이 익사했어',\n",
              " 'tom drowned\\t톰이 물에 빠져 죽었어',\n",
              " 'tom escaped\\t톰이 빠져나갔어',\n",
              " 'tom escaped\\t톰이 빠져나왔어',\n",
              " 'tom exhaled\\t톰이 숨을 내쉬었어',\n",
              " 'tom exhaled\\t톰이 날숨을 쉬었어',\n",
              " 'tom fainted\\t톰이 기절했어',\n",
              " 'tom frowned\\t톰은 얼굴을 찡그렸어',\n",
              " 'tom giggled\\t톰이 피식 웃었어',\n",
              " 'tom giggled\\t톰이 낄낄거렸어',\n",
              " 'tom giggled\\t톰이 키득거렸어',\n",
              " 'tom grinned\\t톰이 씩 웃었어',\n",
              " 'tom grinned\\t톰이 씨익 웃었어',\n",
              " 'tom groaned\\t톰이 앓는 소리를 했어',\n",
              " 'tom inhaled\\t톰이 숨을 들이마셨어',\n",
              " 'tom kneeled\\t톰이 무릎을 꿇었어',\n",
              " 'tom laughed\\t톰이 웃었어',\n",
              " 'toms lying\\t톰은 거짓말 치고 있어',\n",
              " 'turn around\\t돌아',\n",
              " 'walk slowly\\t천천히 걸어',\n",
              " 'we promised\\t우린 약속했어',\n",
              " 'we remember\\t우린 기억하고 있어',\n",
              " 'we survived\\t우리가 살아남았어',\n",
              " 'whats that\\t저건 뭐야',\n",
              " 'work slowly\\t쉬엄쉬엄 일해',\n",
              " 'youre mine\\t넌 내 거야',\n",
              " 'youre mine\\t당신은 나의 것입니다',\n",
              " 'anybody here\\t누구 있어',\n",
              " 'anybody home\\t누구 집에 있어',\n",
              " 'anybody home\\t집에 누군가 있어',\n",
              " 'anybody hurt\\t누가 다쳤어',\n",
              " 'anything new\\t새로운 것이라도 있어',\n",
              " 'be realistic\\t현실적으로 생각해',\n",
              " 'beef please\\t쇠고기요',\n",
              " 'blood is red\\t피는 붉다',\n",
              " 'can i go now\\t이제 가도 되나요',\n",
              " 'come forward\\t앞쪽으로 와',\n",
              " 'come quickly\\t빨리 와',\n",
              " 'come quickly\\t빨리 오세요',\n",
              " 'dont eat it\\t먹지 마',\n",
              " 'drive faster\\t빨리 운전해',\n",
              " 'examine them\\t이것들 조사해봐',\n",
              " 'examine this\\t이걸 조사해봐',\n",
              " 'fish please\\t물고기요',\n",
              " 'ghosts exist\\t유령은 존재해',\n",
              " 'grab a spoon\\t숟가락 집어',\n",
              " 'he succeeded\\t그 사람이 성공했어',\n",
              " 'hes too old\\t그 사람은 너무 늙었어',\n",
              " 'how annoying\\t이렇게 짜증날 수가',\n",
              " 'how annoying\\t이렇게나 짜증나다니',\n",
              " 'how arrogant\\t이렇게나 건방지다니',\n",
              " 'how horrible\\t이렇게나 끔찍하다니',\n",
              " 'i apologized\\t난 사과했어',\n",
              " 'i dont know\\t나는 몰라요',\n",
              " 'i hate liars\\t난 거짓말쟁이가 싫어',\n",
              " 'i need money\\t돈이 필요해요',\n",
              " 'i understand\\t이해해',\n",
              " 'i understood\\t이해했어',\n",
              " 'im addicted\\t난 중독자야',\n",
              " 'im bleeding\\t나 피나',\n",
              " 'is that okay\\t괜찮은 거예요',\n",
              " 'is this mine\\t이거 내꺼니',\n",
              " 'is this wine\\t이게 와인이야',\n",
              " 'itll be hot\\t더워질거야',\n",
              " 'its suicide\\t자살입니다',\n",
              " 'its too old\\t그건 너무 낡았어',\n",
              " 'its too old\\t그건 너무 오래되었어',\n",
              " 'just keep it\\t그냥 그거 가져',\n",
              " 'keep dancing\\t계속 춤춰',\n",
              " 'keep dancing\\t계속 춤추고 있어봐',\n",
              " 'keep digging\\t계속 땅 파고 있어봐',\n",
              " 'keep digging\\t계속 땅 파',\n",
              " 'keep driving\\t계속 운전하고 있어봐',\n",
              " 'keep focused\\t계속 집중하고 있어봐',\n",
              " 'keep looking\\t계속 보고 있어봐',\n",
              " 'keep reading\\t계속 읽어',\n",
              " 'keep singing\\t계속 노래하고 있어',\n",
              " 'keep singing\\t계속 노래해',\n",
              " 'keep smiling\\t계속 미소지어',\n",
              " 'keep smiling\\t계속 웃어',\n",
              " 'keep smiling\\t계속 웃고 있어봐',\n",
              " 'keep talking\\t계속 말해',\n",
              " 'keep talking\\t계속 말하고 있어봐',\n",
              " 'move quietly\\t조용히 움직여',\n",
              " 'nobody asked\\t아무도 안 물어봤어',\n",
              " 'nobody knows\\t아무도 몰라',\n",
              " 'please hurry\\t서둘러 주세요',\n",
              " 'please leave\\t제발 떠나 줘',\n",
              " 'please relax\\t제발 진정해',\n",
              " 'please smile\\t웃어줘',\n",
              " 'she was busy\\t그는 바빴다',\n",
              " 'someone came\\t누군가 왔어',\n",
              " 'stop reading\\t그만 읽어',\n",
              " 'stop smoking\\t담배 피지 마',\n",
              " 'stop staring\\t그만 쳐다봐',\n",
              " 'stop talking\\t그만 말해',\n",
              " 'stop whining\\t그만 흐느껴',\n",
              " 'stop yelling\\t그만 소리쳐',\n",
              " 'they escaped\\t그 사람들은 도망 쳤어',\n",
              " 'they laughed\\t그 사람들 웃었어',\n",
              " 'they refused\\t그 사람들은 거절했어',\n",
              " 'they relaxed\\t그 사람들은 진정했어',\n",
              " 'tom answered\\t톰이 대답했어',\n",
              " 'tom approved\\t톰이 승낙했어',\n",
              " 'tom chuckled\\t톰이 싱긋 웃었어',\n",
              " 'tom chuckled\\t톰이 빙그레 웃었어',\n",
              " 'tom enlisted\\t톰이 입대했어',\n",
              " 'tom finished\\t톰이 끝냈어',\n",
              " 'tom flinched\\t톰이 움찔했어',\n",
              " 'tom grumbled\\t톰이 궁시렁거렸어',\n",
              " 'tom grumbled\\t톰이 투덜거렸어',\n",
              " 'tom insisted\\t톰이 주장했어',\n",
              " 'tom is eager\\t톰은 열성적이야',\n",
              " 'tom is quick\\t톰은 빨라',\n",
              " 'tom is quick\\t톰은 날렵해',\n",
              " 'tom is silly\\t톰은 실없어',\n",
              " 'tom is sober\\t톰은 제정신이야',\n",
              " 'tom is sober\\t톰은 멀쩡해',\n",
              " 'tom listened\\t톰이 듣고 있었어',\n",
              " 'tom shrugged\\t톰이 어깨를 으쓱했어',\n",
              " 'tom whistled\\t톰이 휘파람 불었어',\n",
              " 'unbelievable\\t설마',\n",
              " 'we apologize\\t우리가 사과할게',\n",
              " 'we dont lie\\t우리는 거짓말을 하지 않아요',\n",
              " 'we dont lie\\t우린 거짓말 안해',\n",
              " 'we overslept\\t우린 늦잠잤어',\n",
              " 'we succeeded\\t우린 성공했어',\n",
              " 'we succeeded\\t우리는 성공했어',\n",
              " 'were inside\\t우리 안에 들어와 있어',\n",
              " 'were inside\\t우린 안에 있어요',\n",
              " 'welcome back\\t어서 와',\n",
              " 'welcome home\\t어서와',\n",
              " 'what is that\\t저것은 무엇입니까',\n",
              " 'any questions\\t질문 있어',\n",
              " 'any questions\\t아무 질문이라도',\n",
              " 'can i ask why\\t이유를 물어봐도 돼',\n",
              " 'cats are cute\\t고양이는 귀여워',\n",
              " 'come tomorrow\\t내일 와',\n",
              " 'eat something\\t뭔가 먹어',\n",
              " 'everyone dies\\t누구나 죽어',\n",
              " 'flowers bloom\\t꽃이 피네',\n",
              " 'grab the rope\\t로프를 잡으세요',\n",
              " 'he is too old\\t그 사람은 너무 늙었어',\n",
              " 'hes autistic\\t그 사람은 자폐성향이 있어',\n",
              " 'how beautiful\\t이렇게나 아름다울 수가',\n",
              " 'how wonderful\\t이렇게 멋질 수가',\n",
              " 'i am homesick\\t나 향수병 걸렸어',\n",
              " 'i cant sleep\\t잠이 와',\n",
              " 'i feel guilty\\t죄책감이 들어',\n",
              " 'i feel lonely\\t외로워',\n",
              " 'i got engaged\\t나 약혼했어',\n",
              " 'i hate myself\\t나는 내 자신이 싫어',\n",
              " 'i like horses\\t나는 말을 좋아해',\n",
              " 'i like winter\\t난 겨울이 좋아',\n",
              " 'i miss my cat\\t난 내 고양이가 그리워',\n",
              " 'i smell blood\\t피 냄새가 납니다',\n",
              " 'i use firefox\\t나는 파이어폭스를 사용해',\n",
              " 'i want to die\\t죽고 싶어요',\n",
              " 'i want to die\\t죽고 싶어',\n",
              " 'ill kill him\\t나는 그를 죽일 것이다',\n",
              " 'im at school\\t난 학교에 있어',\n",
              " 'im depressed\\t우울해',\n",
              " 'im religious\\t난 신앙이 있어',\n",
              " 'is my time up\\t내 시간 끝났어',\n",
              " 'is that blood\\t그거 피야',\n",
              " 'keep fighting\\t계속 싸워',\n",
              " 'my head hurts\\t머리가 아파요',\n",
              " 'my names tom\\t제 이름은 톰입니다',\n",
              " 'please listen\\t제발 좀 들어',\n",
              " 'quiet please\\t조용히 해줘',\n",
              " 'quit gambling\\t도박 그만해',\n",
              " 'say something\\t아무 말이나 해봐',\n",
              " 'she overslept\\t그 사람은 늦잠잤어',\n",
              " 'she was naive\\t그는 순진했다',\n",
              " 'she was young\\t그는 어렸다',\n",
              " 'show yourself\\t너의 모습을 드러내',\n",
              " 'show yourself\\t네 모습을 보여줘',\n",
              " 'speak clearly\\t분명하게 말해',\n",
              " 'start singing\\t노래 시작해',\n",
              " 'stay positive\\t긍정적으로 있어',\n",
              " 'stop babbling\\t그만 떠들어',\n",
              " 'stop fighting\\t그만 싸워',\n",
              " 'stop laughing\\t그만 웃어',\n",
              " 'stop shooting\\t총 그만 쏴',\n",
              " 'stop worrying\\t그만 걱정해',\n",
              " 'stop worrying\\t걱정 그만해',\n",
              " 'thanks anyway\\t어쨌든 고마워',\n",
              " 'thats my cat\\t저거 내 고양이야',\n",
              " 'thats my dog\\t이건 내 강아지야',\n",
              " 'thats not it\\t아니야',\n",
              " 'they screamed\\t그 사람들은 비명을 질렀어',\n",
              " 'tom confessed\\t톰이 자백했어',\n",
              " 'tom graduated\\t톰이 졸업했어',\n",
              " 'tom has a cat\\t톰은 고양이를 키우고 있어',\n",
              " 'tom hesitated\\t톰이 주저했어',\n",
              " 'tom hesitated\\t톰이 머뭇거렸어',\n",
              " 'tom is honest\\t톰은 정직하다',\n",
              " 'tom overslept\\t톰은 늦잠잤어',\n",
              " 'watch closely\\t가까이서 봐',\n",
              " 'we can buy it\\t이건 우리가 살 수 있어',\n",
              " 'we understand\\t우린 이해해',\n",
              " 'we want peace\\t우리는 평화를 원합니다',\n",
              " 'were too old\\t우린 너무 늙었어',\n",
              " 'you work hard\\t너는 열심히 일을 한다',\n",
              " 'apples are red\\t사과는 빨개',\n",
              " 'autumn is here\\t가을이 되었습니다',\n",
              " 'autumn is here\\t가을이 왔어요',\n",
              " 'can i help you\\t제가 좀 도와 드릴까요',\n",
              " 'cats are great\\t고양이들은 멋져',\n",
              " 'do you hear me\\t제 말이 들리세요',\n",
              " 'everybody knew\\t모두가 알고 있었어',\n",
              " 'everybody left\\t모두 떠났어',\n",
              " 'everybody lies\\t누구나 거짓말 해',\n",
              " 'everyone stood\\t모두 일어섰어',\n",
              " 'everyone stood\\t모두가 일어섰어',\n",
              " 'happy new year\\t새해 복 많이 받아',\n",
              " 'he was hard up\\t그는 돈에 쪼들리고 있었다',\n",
              " 'hello everyone\\t모두들 안녕',\n",
              " 'i dont buy it\\t못 믿어',\n",
              " 'i feel relaxed\\t마음이 편안하다',\n",
              " 'i like reading\\t독서를 좋아합니다',\n",
              " 'i like to work\\t나는 일하기 좋아해',\n",
              " 'i love lasagna\\t저는 라자냐를 좋아해요',\n",
              " 'i love my home\\t난 내 집이 좋아',\n",
              " 'i study korean\\t한국말을 공부합니다',\n",
              " 'ill find them\\t내가 찾아 볼게',\n",
              " 'im very sorry\\t정말 미안해',\n",
              " 'im very sorry\\t정말 죄송합니다',\n",
              " 'its not funny\\t안 재밌어',\n",
              " 'its so simple\\t정말 간단해',\n",
              " 'keep tom there\\t톰은 여기에 두세요',\n",
              " 'keep listening\\t계속 들어',\n",
              " 'keep searching\\t계속 찾아봐',\n",
              " 'keep searching\\t계속 찾고 있어봐',\n",
              " 'louder please\\t좀더 큰소리로 부탁해',\n",
              " 'my name is tom\\t제 이름은 톰입니다',\n",
              " 'only god knows\\t신만이 아실 것입니다',\n",
              " 'please proceed\\t진행해줘',\n",
              " 'read this book\\t이 책 읽어',\n",
              " 'read this book\\t이 책을 읽으세요',\n",
              " 'science is fun\\t과학은 재밌어',\n",
              " 'smoking stinks\\t담배 냄새 나',\n",
              " 'someone called\\t누군가 불렀어',\n",
              " 'sorry im late\\t늦어서 미안해',\n",
              " 'stop grumbling\\t그만 투덜거려',\n",
              " 'tell everybody\\t모두한테 말해',\n",
              " 'tell everybody\\t모두에게 말해',\n",
              " 'thats suicide\\t그것은 자살입니다',\n",
              " 'the cat meowed\\t고양이가 야옹하고 울었어',\n",
              " 'the ice melted\\t얼음이 녹았어',\n",
              " 'they quarreled\\t그사람들 싸웠어',\n",
              " 'ticket please\\t티켓 부탁해',\n",
              " 'tom apologized\\t톰이 사과했어',\n",
              " 'tom hates cats\\t톰은 고양이를 싫어해',\n",
              " 'tom is abusive\\t톰은 폭력적이야',\n",
              " 'tom is awesome\\t톰은 정말 멋져',\n",
              " 'tom is too old\\t톰은 너무 늙었어',\n",
              " 'tom went there\\t톰이 거기로 갔어',\n",
              " 'watch yourself\\t조심해',\n",
              " 'we need change\\t우린 변화가 필요해',\n",
              " 'we surrendered\\t우린 항복했어',\n",
              " 'we volunteered\\t우린 자원해서 했어',\n",
              " 'you look smart\\t너 똑똑해 보여',\n",
              " 'you scared tom\\t넌 톰을 무섭게 했어',\n",
              " 'youre too old\\t넌 너무 늙었어',\n",
              " 'behave yourself\\t처신 잘해',\n",
              " 'boil some water\\t물 좀 끓여',\n",
              " 'can you help me\\t저를 좀 도와 주실래요',\n",
              " 'congratulations\\t축하해',\n",
              " 'congratulations\\t축하해요',\n",
              " 'did tom do that\\t톰이 그랬대',\n",
              " 'did tom look ok\\t톰은 괜찮아 보였어',\n",
              " 'do you like rap\\t랩 좋아해요',\n",
              " 'do you like rap\\t랩 좋아해',\n",
              " 'dont lie to me\\t내게 거짓말 하지 마',\n",
              " 'dont lie to me\\t제게 거짓말 하지 마세요',\n",
              " 'dont lie to us\\t우리에게 거짓말 하지 마',\n",
              " 'dont lie to us\\t저희에게 거짓말 하지 마세요',\n",
              " 'drive carefully\\t운전 조심하세요',\n",
              " 'drive carefully\\t운전 조심해',\n",
              " 'everybody knows\\t모두가 알고 있어',\n",
              " 'everyone dreams\\t누구나 꿈을 꿔',\n",
              " 'everyone looked\\t모두들 쳐다봤어',\n",
              " 'everyone prayed\\t모두 기도했어',\n",
              " 'everyone prayed\\t모두들 기도했어',\n",
              " 'everyone smiled\\t모두들 미소지었어',\n",
              " 'everyone waited\\t모두들 기다렸어',\n",
              " 'green suits you\\t초록색이 너한테 어울려',\n",
              " 'have a good day\\t좋은하루 되세요',\n",
              " 'he came at dawn\\t그는 새벽에 왔다',\n",
              " 'he is depressed\\t그는 우울하다',\n",
              " 'he loves trains\\t그는 열차를 좋아한다',\n",
              " 'how fascinating\\t이렇게 매력적일 수가',\n",
              " 'how fascinating\\t이렇게나 매력적이라니',\n",
              " 'how interesting\\t이렇게나 흥미롭다니',\n",
              " 'i caught a cold\\t감기 걸렸어',\n",
              " 'i caught a cold\\t감기에 걸렸어',\n",
              " 'i didnt scream\\t난 비명을 지르지 않았어',\n",
              " 'i felt excluded\\t난 소외감을 느꼈어',\n",
              " 'i hate funerals\\t장례식이 싫어',\n",
              " 'i hate my voice\\t나는 내 목소리가 싫다',\n",
              " 'i know tom well\\t나는 톰을 잘 안다',\n",
              " 'i learned a lot\\t나는 많이 배웠어',\n",
              " 'i misunderstood\\t난 오해했어',\n",
              " 'i was convicted\\t나는 유죄 판결을 받았다',\n",
              " 'i work at a zoo\\t나는 동물원에서 일해',\n",
              " 'im embarrassed\\t창피해',\n",
              " 'im heartbroken\\t제 마음이 아파요',\n",
              " 'im not sulking\\t나 삐친 거 아니야',\n",
              " 'is this ethical\\t이거 윤리적이야',\n",
              " 'it doesnt hurt\\t아프지 않아',\n",
              " 'its very humid\\t상당히 습하다',\n",
              " 'keep the change\\t잔돈은 가지세요',\n",
              " 'might i come in\\t내가 들어와도 될까',\n",
              " 'my blood boiled\\t내 피가 끓었다',\n",
              " 'my cat is black\\t내 고양이는 검은색 고양이야',\n",
              " 'please continue\\t계속 해줘',\n",
              " 'she disappeared\\t그 사람이 사라졌어',\n",
              " 'she disappeared\\t그가 사라졌어',\n",
              " 'she was my boss\\t그는 내 상사였다',\n",
              " 'shes a trainee\\t그녀는 연습생이다',\n",
              " 'shes depressed\\t그녀는 우울하다',\n",
              " 'someone coughed\\t누군가 기침했어',\n",
              " 'sorry im late\\t늦어서 미안합니다',\n",
              " 'that was stupid\\t그건 멍청했어',\n",
              " 'thats a pencil\\t그거 연필이야',\n",
              " 'thats horrible\\t끔찍하네',\n",
              " 'the bag is full\\t가방이 꽉 찼습니다',\n",
              " 'the cat is lazy\\t고양이는 게을러',\n",
              " 'there was blood\\t피가 있었다',\n",
              " 'they believe me\\t그 사람들은 날 믿어',\n",
              " 'they want peace\\t그들은 평화를 원한다',\n",
              " 'they were naive\\t걔들이 순진했어',\n",
              " 'this is suicide\\t이것은 자살입니다',\n",
              " 'tom didnt vote\\t톰은 투표 안 했어',\n",
              " 'tom disappeared\\t톰이 사라졌어',\n",
              " 'tom got retired\\t톰은 은퇴했어',\n",
              " 'tom is a runner\\t톰은 달리기 선수야',\n",
              " 'tom volunteered\\t톰이 자웠했어',\n",
              " 'tom was too old\\t톰은 너무 늙었어',\n",
              " 'watch carefully\\t잘 봐',\n",
              " 'we need experts\\t우리에겐 전문가가 필요해',\n",
              " 'what time is it\\t몇시 입니까',\n",
              " 'why do we dream\\t왜 우리는 꿈을 꿔',\n",
              " 'write something\\t뭔가 써',\n",
              " 'youre a genius\\t너 천재구나',\n",
              " 'are you busy now\\t지금 바빠',\n",
              " 'are you in there\\t너 거기 있어',\n",
              " 'are you sleeping\\t자고 있어',\n",
              " 'are you studying\\t공부하고 계십니까',\n",
              " 'call this number\\t이 번호로 전화해',\n",
              " 'choose carefully\\t신중하게 골라',\n",
              " 'come immediately\\t즉시 와',\n",
              " 'do you like fish\\t생선 좋아해요',\n",
              " 'do you like fish\\t생선 좋아해',\n",
              " 'do you like fish\\t물고기 좋아하나요',\n",
              " 'do you like fish\\t물고기 좋아해',\n",
              " 'drink some water\\t물 좀 마셔',\n",
              " 'everybody smiled\\t모두 웃었어',\n",
              " 'everybody smiled\\t모두 미소지었어',\n",
              " 'everyone changes\\t누구나 바뀌어',\n",
              " 'everyone laughed\\t모두가 웃었어',\n",
              " 'give me my sword\\t내 검을 줘',\n",
              " 'give me the file\\t나한테 파일 줘',\n",
              " 'he loves singing\\t그는 노래하기를 좋아한다',\n",
              " 'her hair is long\\t그녀의 머리카락은 길어',\n",
              " 'her hair is long\\t그녀의 머리카락은 길다',\n",
              " 'here is the bill\\t여기 계산서 입니다',\n",
              " 'his face was red\\t그 사람 얼굴은 빨갰어',\n",
              " 'how embarrassing\\t이렇게 창피할 수가',\n",
              " 'i bought a horse\\t난 말 한 마리를 샀어',\n",
              " 'i dont envy you\\t난 네가 부럽지 않아',\n",
              " 'i felt very safe\\t나는 매우 안전하다고 느꼈다',\n",
              " 'i like languages\\t언어를 좋아합니다',\n",
              " 'i like languages\\t언어가 좋습니다',\n",
              " 'i lost my wallet\\t지갑을 잃어버렸어',\n",
              " 'i may be too old\\t난 너무 늙었을지도 몰라',\n",
              " 'i may be too old\\t난 아마 너무 늙었어',\n",
              " 'i missed the bus\\t버스를 놓쳤어요',\n",
              " 'i said im sorry\\t미안하다고 했잖아',\n",
              " 'i said im sorry\\t미안하다고 했잖아요',\n",
              " 'im dead serious\\t난 절대 농담하는게 아냐',\n",
              " 'im disorganized\\t나는 체계적이지 못하다',\n",
              " 'im not suicidal\\t나는 자살하고 싶지 않다',\n",
              " 'im your teacher\\t난 네 선생이다',\n",
              " 'is that a spider\\t이거 거미야',\n",
              " 'is the bank open\\t은행 문 열었어요',\n",
              " 'is the bank open\\t은행 해요',\n",
              " 'it could be true\\t진짜일 수도 있어',\n",
              " 'it was a mistake\\t그건 실수였어',\n",
              " 'its a full moon\\t보름달이야',\n",
              " 'its nearly dark\\t거의 어두워지고 있어',\n",
              " 'its really loud\\t정말 시끄럽네',\n",
              " 'listen carefully\\t주의깊게 들어',\n",
              " 'look at the moon\\t달을 봐',\n",
              " 'nobodys perfect\\t그 누구도 완벽하지 않는다',\n",
              " 'nobodys perfect\\t완벽한 사람은 없어',\n",
              " 'nothing happened\\t아무일도 없었어',\n",
              " 'she is beautiful\\t그녀는 아름답다',\n",
              " 'somebody laughed\\t누군가 웃었어',\n",
              " 'someone has died\\t누군가 죽었어',\n",
              " 'someone screamed\\t누군가 비명을 질렀어',\n",
              " 'stop apologizing\\t그만 사과해',\n",
              " 'stop complaining\\t그만 불평해',\n",
              " 'that is a pencil\\t그거 연필이야',\n",
              " 'that made me cry\\t그것 때문에 울었다',\n",
              " 'thats very kind\\t참 친절하구나',\n",
              " 'the kids love it\\t아이들이 좋아해',\n",
              " 'there arent any\\t없다',\n",
              " 'they disappeared\\t그들은 사라졌어',\n",
              " 'they disappeared\\t그 사람들은 사라졌어',\n",
              " 'theyre amateurs\\t걔네 초짜야',\n",
              " 'think about this\\t생각 좀 해봐',\n",
              " 'this is a flower\\t이건 꽃이야',\n",
              " 'this seats free\\t이 자리 비었어',\n",
              " 'tom has kids now\\t톰한텐 이제 애들이 있어',\n",
              " 'tom is a manager\\t톰은 관리자야',\n",
              " 'tom is a nominee\\t톰은 후보야',\n",
              " 'tom is an orphan\\t톰은 고아야',\n",
              " 'tom is fantastic\\t톰은 환상적이야',\n",
              " 'tom is fast too\\t톰도 빨라',\n",
              " 'tom is real busy\\t톰은 진짜 바빠',\n",
              " 'tom is worked up\\t톰은 흥분해 있다',\n",
              " 'tom isnt a liar\\t톰은 거짓말쟁이가 아니야',\n",
              " 'tom isnt skinny\\t톰은 마르지 않았다',\n",
              " 'watch me closely\\t나를 가까이서 봐',\n",
              " 'water the plants\\t식물에 물을 주세요',\n",
              " 'we cant give up\\t우린 포기할 수 없어',\n",
              " 'we cant give up\\t포기할 수 없어요',\n",
              " 'wed been warned\\t우린 경고 받았었어',\n",
              " 'what do you like\\t무엇을 좋아하세요',\n",
              " 'what do you like\\t뭘 좋아해',\n",
              " 'what do you like\\t뭐가 좋아',\n",
              " 'where is the cat\\t고양이는 어딨어',\n",
              " 'you made tom cry\\t네가 톰을 울렸어',\n",
              " 'youll regret it\\t너 후회할거야',\n",
              " 'youre shivering\\t너 떨고 있네',\n",
              " 'your plan failed\\t네 계획은 실패했어',\n",
              " 'admission is free\\t입장은 무료야',\n",
              " 'can i borrow this\\t이거 좀 빌려 줄래',\n",
              " 'can you handle it\\t잘 해낼 수 있니',\n",
              " 'champagne please\\t샴폐인 좀',\n",
              " 'champagne please\\t샴폐인 부탁해',\n",
              " 'champagne please\\t샴폐인 주세요',\n",
              " 'come and see this\\t와서 이것좀 봐',\n",
              " 'do you have proof\\t증거 있어',\n",
              " 'do you work alone\\t혼자서 일해',\n",
              " 'does tom like you\\t톰은 너 좋아해',\n",
              " 'dont be so silly\\t실없게 굴지마',\n",
              " 'dont even try it\\t시도조차 하지마',\n",
              " 'everybody laughed\\t전부 웃었어',\n",
              " 'everyone screamed\\t모두 비명을 질렀어',\n",
              " 'everyone survived\\t모두 살아남았어',\n",
              " 'everyone survived\\t모두가 살아남았어',\n",
              " 'everyone survived\\t모두들 살아남았어',\n",
              " 'exercise outdoors\\t밖에서 운동해',\n",
              " 'god bless you all\\t신의 가호가 있길',\n",
              " 'goodnight mother\\t엄마 잘자',\n",
              " 'he died yesterday\\t그는 어제 죽었어',\n",
              " 'her nails are red\\t그 사람의 손톱은 빨간색이야',\n",
              " 'i believe in love\\t난 사랑을 믿는다',\n",
              " 'i can go tomorrow\\t난 내일 갈 수 있어',\n",
              " 'i cant walk fast\\t난 빠르게 걸을 수 없어',\n",
              " 'i cant walk fast\\t난 빠른 걸음을 할 수 없어',\n",
              " 'i cant walk fast\\t난 빠른 걸음은 할 수 없어',\n",
              " 'i doubted my eyes\\t난 내 눈을 의심했어',\n",
              " 'i feel really sad\\t나 정말 슬퍼',\n",
              " 'i got up at seven\\t난 일곱 시에 일어났어',\n",
              " 'i have a headache\\t머리가 아파요',\n",
              " 'i heard tom laugh\\t난 톰이 웃는 걸 들었어',\n",
              " 'i heard tom shout\\t난 톰이 소리지르는 걸 들었어',\n",
              " 'i kicked tom hard\\t내가 톰을 세게 걷어찼어',\n",
              " 'i like watermelon\\t난 수박 좋아해',\n",
              " 'i met tom outside\\t난 톰을 밖에서 봤어',\n",
              " 'i saw tom waiting\\t톰이 기다리는 걸 봤어',\n",
              " 'i saw tom working\\t난 톰이 일하는 걸 봤어',\n",
              " 'i sent tom a text\\t난 톰한테 문자 보냈어',\n",
              " 'i shook tom awake\\t난 톰을 흔들어 깨웠어',\n",
              " 'i stole toms car\\t내가 톰의 자동차를 훔쳤어',\n",
              " 'i taught tom golf\\t난 톰한테 골프를 가르쳤어',\n",
              " 'i texted tom back\\t난 톰한테 답장했어',\n",
              " 'i told tom to lie\\t난 톰한테 거짓말 하라고 말했어',\n",
              " 'i used toms idea\\t톰의 아이디어를 썼어',\n",
              " 'i used to be poor\\t난 가난했었어',\n",
              " 'i warned you once\\t난 널 한 번 경고했어',\n",
              " 'i was intoxicated\\t술 취했었다',\n",
              " 'i was quite lucky\\t난 꽤 운이 좋았었어',\n",
              " 'i watched tom eat\\t톰이 먹는 걸 지켜봤어',\n",
              " 'i watched a movie\\t나는 영화를 봤어',\n",
              " 'i wont permit it\\t나는 그것을 허락하지 않을거야',\n",
              " 'id be devastated\\t난 피폐해질 거야',\n",
              " 'ill go by subway\\t지하철로 갈게',\n",
              " 'im a human being\\t난 인간이야',\n",
              " 'im a patient man\\t난 참을성이 있는 남자야',\n",
              " 'im a simple girl\\t저는 단순한 소녀에요',\n",
              " 'im extremely shy\\t난 극히 낯을 가려',\n",
              " 'im in charge now\\t이제 내가 책임을 지고 있다',\n",
              " 'im not an addict\\t난 중독자 아니야',\n",
              " 'ive fed the fish\\t물고기한테 먹이를 줬어',\n",
              " 'its a dictionary\\t이것은 사전이다',\n",
              " 'its just a dream\\t그냥 꿈일 뿐이야',\n",
              " 'leave me in peace\\t저를 내버려 두세요',\n",
              " 'lets play a game\\t게임 한판 하자',\n",
              " 'life is too short\\t삶은 너무 짧네',\n",
              " 'look at the clock\\t시계를 보세요',\n",
              " 'may god bless you\\t신께서 당신을 축복하시길',\n",
              " 'no music no life\\t음악이 없으면 인생도 없어',\n",
              " 'no one was killed\\t아무도 죽지 않았어',\n",
              " 'nobody is perfect\\t그 누구도 완벽하지 않는다',\n",
              " 'please reconsider\\t다시 한 번 고려해줘',\n",
              " 'save it for later\\t나중을 위해 아껴둬',\n",
              " 'she isnt married\\t그 사람은 아직 결혼하지 않았어',\n",
              " 'she isnt married\\t그 사람은 미혼이야',\n",
              " 'she isnt married\\t그는 결혼하지 않았다',\n",
              " 'she isnt married\\t그는 미혼이다',\n",
              " 'she loves singing\\t그녀는 노래하기를 좋아한다',\n",
              " 'something changed\\t무언가 바뀌었어',\n",
              " 'stop overreacting\\t과민반응 그만해',\n",
              " 'take the medicine\\t그 약을 먹으세요',\n",
              " 'they got addicted\\t그 사람들 중독되었어',\n",
              " 'theyre all liars\\t너희 모두 거짓말쟁이다',\n",
              " 'tom hates spiders\\t톰은 거미를 싫어해',\n",
              " 'tom is a bit late\\t톰이 좀 늦네',\n",
              " 'tom is not a liar\\t톰은 거짓말쟁이가 아니야',\n",
              " 'tom made mary mad\\t톰은 메리를 화나게 했어',\n",
              " 'tom moves quickly\\t톰은 빨리 움직이네',\n",
              " 'tom seemed sleepy\\t톰은 졸려 보였어',\n",
              " 'tom vomited blood\\t톰은 피를 토했다',\n",
              " 'tom won the match\\t톰은 경기에서 이겼다',\n",
              " 'turn on the radio\\t라디오를 켜세요',\n",
              " 'we only take cash\\t우리는 현금만 가져갈 거야',\n",
              " 'what a tacky idea\\t참으로 조잡한 발상이네',\n",
              " 'whats this smell\\t이거 무슨 냄새지',\n",
              " 'where is the bank\\t그 은행은 어디에 있어요',\n",
              " 'where is the bank\\t그 은행은 어디 있어',\n",
              " 'why are you lying\\t왜 거짓말 하는거야',\n",
              " 'you are beautiful\\t아름다우시네요',\n",
              " 'you can have mine\\t너는 내 것을 가질 수 있다',\n",
              " 'youll get lonely\\t넌 외로워질거야',\n",
              " 'youre both liars\\t너희 둘 다 거짓말쟁이다',\n",
              " 'your lips are red\\t네 입술 빨개',\n",
              " 'a cat is not human\\t고양이는 인간이 아니야',\n",
              " 'ask tom for advice\\t톰에게 조언을 구해봐',\n",
              " 'can i see this one\\t이것 좀 보여주실래요',\n",
              " 'dont be so greedy\\t그렇게 욕심부리지 마',\n",
              " 'dont get me wrong\\t오해하지 마',\n",
              " 'drink to my health\\t건강을 위해 건배',\n",
              " 'everyone hesitated\\t모두 주저했어',\n",
              " 'everything changed\\t모든 것이 변했어',\n",
              " 'everything stopped\\t모든 것이 멈췄어',\n",
              " 'excuse me a minute\\t잠깐 실례합니다',\n",
              " 'he kept me waiting\\t그는 나를 기다리게 했다',\n",
              " 'i baked tom a cake\\t나는 톰에게 케이크를 구워줬다',\n",
              " 'i didnt know that\\t그건 몰랐어요',\n",
              " 'i dont understand\\t이해가 안돼',\n",
              " 'i feel very guilty\\t나는 큰 죄책감을 느끼고 있어',\n",
              " 'i have a black dog\\t나는 검은색 강아지를 키우고 있어',\n",
              " 'i have to go alone\\t난 혼자서 가야해',\n",
              " 'i knew i would win\\t내가 이길 거라는 걸 알고 있었어',\n",
              " 'i love red parrots\\t난 빨간색 앵무새가 좋아',\n",
              " 'i love this school\\t난 이 학교를 좋아해',\n",
              " 'i need to warn tom\\t나는 톰에게 경고해야 해',\n",
              " 'i often read books\\t저는 자주 책을 읽습니다',\n",
              " 'i really like snow\\t나는 눈이 정말 좋아요',\n",
              " 'i really like snow\\t난 눈이 정말 좋아',\n",
              " 'i tried not to cry\\t난 울지 않으려고 노력했어',\n",
              " 'i tried not to cry\\t나는 울지 않으려고 했다',\n",
              " 'i want to be happy\\t나는 행복해지고 싶어',\n",
              " 'i wanted red shoes\\t빨간색 신발을 원했어',\n",
              " 'i was born in 2013\\t난 2013년에 태어났어',\n",
              " 'i wasnt even here\\t난 심지어 여기에 없었어',\n",
              " 'i wasnt even here\\t난 심지어 여기 없었어',\n",
              " 'ill be there soon\\t곧 갈게',\n",
              " 'im like my father\\t나는 우리 아빠같아',\n",
              " 'im like my father\\t나는 우리 아빠를 닮았어',\n",
              " 'im still a member\\t난 아직도 회원이야',\n",
              " 'im very depressed\\t난 아주 우울해',\n",
              " 'ive caught a cold\\t감기 걸렸어',\n",
              " 'is that real blood\\t그거 진짜 피야',\n",
              " 'it must be a virus\\t이건 틀림없이 바이러스야',\n",
              " 'it was toms fault\\t이건 톰 탓이야',\n",
              " 'lets move the bed\\t침대를 옮기자',\n",
              " 'life is a delusion\\t인생은 허상이야',\n",
              " 'life is never easy\\t사는게 쉽지가 않아',\n",
              " 'no one believed me\\t아무도 날 믿지 않았어',\n",
              " 'return immediately\\t즉시 돌아와',\n",
              " 'she has small feet\\t그녀는 발이 작다',\n",
              " 'some water please\\t물 좀 주세요 제발',\n",
              " 'something happened\\t무언가 일어났어',\n",
              " 'something happened\\t무언가 생겼어',\n",
              " 'stop looking at me\\t나좀 그만 봐',\n",
              " 'thats unavoidable\\t그건 피할 수 없어',\n",
              " 'the light went out\\t전등이 꺼졌다',\n",
              " 'the night was cold\\t그날 밤은 추웠어',\n",
              " 'the room was quiet\\t방은 조용했어',\n",
              " 'the spider is dead\\t거미가 죽었어',\n",
              " 'they have no proof\\t그들에게는 증거가 없었다',\n",
              " 'they have no proof\\t그들한텐 증거가 없어',\n",
              " 'tie your shoelaces\\t신발끈을 묶으세요',\n",
              " 'tom bought a horse\\t톰은 말을 샀어',\n",
              " 'tom could be lying\\t톰이 거짓말을 하고 있을 수 있다',\n",
              " 'tom died on monday\\t톰은 월요일에 죽었어',\n",
              " 'tom got very happy\\t톰은 아주 행복해졌어',\n",
              " 'tom has aspergers\\t톰은 아스퍼거야',\n",
              " 'tom hired a lawyer\\t톰이 변호사를 고용했어',\n",
              " 'tom hired a lawyer\\t톰은 변호사를 고용했다',\n",
              " 'tom is a rough man\\t톰은 거친 남자야',\n",
              " 'tom is a timid kid\\t톰은 소심한 애야',\n",
              " 'tom is openminded\\t톰은 개방적이야',\n",
              " 'tom is quite a guy\\t톰은 꽤 남자다워',\n",
              " 'tom made me a cake\\t톰이 날 위해 케이크를 만들었어',\n",
              " 'tom never helps me\\t톰은 날 절대 안 도와줘',\n",
              " 'tom said hes weak\\t톰은 자신이 약하다고 말했다',\n",
              " 'tom was astonished\\t톰은 깜짝 놀랐다',\n",
              " 'tom wont be bored\\t톰은 지루해하지 않을 거야',\n",
              " 'we were retreating\\t우리는 후퇴하고 있었다',\n",
              " 'were still eating\\t우린 아직도 먹는 중이야',\n",
              " 'were still eating\\t우린 아직도 먹고 있어',\n",
              " 'weve been worried\\t계속 걱정했어',\n",
              " 'when will you come\\t언제쯤 올거야',\n",
              " 'wheres tom hiding\\t톰은 어디에 숨어 있어',\n",
              " 'whose book is this\\t이것은 누구의 책입니까',\n",
              " 'wifi is available\\t와이파이 돼',\n",
              " 'youre very astute\\t넌 참 눈치가 빠르네',\n",
              " 'am i older than you\\t내가 너보다 나이가 많아',\n",
              " 'are you still alone\\t너 아직도 혼자야',\n",
              " 'can i have this cup\\t이 컵 가져도 돼요',\n",
              " 'chivalry isnt dead\\t기사도는 죽지 않았다',\n",
              " 'do you go to school\\t학교에 다녀',\n",
              " 'do you have a house\\t집 있어요',\n",
              " 'do you like english\\t영어 좋아해요',\n",
              " 'do you like english\\t영어 좋아해',\n",
              " 'do you like cooking\\t요리 좋아해요',\n",
              " 'do you like cooking\\t요리 좋아해',\n",
              " 'do you like singing\\t노래하는 거 좋아해요',\n",
              " 'do you like singing\\t노래하는 거 좋아해',\n",
              " 'dont you like cats\\t고양이를 좋아하지 않아',\n",
              " 'dreams do come true\\t꿈은 이루어질 거야',\n",
              " 'everybody loves her\\t모두 그녀를 사랑한다',\n",
              " 'everybody loves him\\t모두가 그를 사랑한다',\n",
              " 'everythings normal\\t모든 것이 정상이야',\n",
              " 'hes in his fifties\\t그 사람은 오십 대야',\n",
              " 'his dream came true\\t그 사람의 꿈은 실현되었어',\n",
              " 'how may i serve you\\t어떻게 도와드릴까요',\n",
              " 'i acted like a fool\\t나는 바보같이 굴었어',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l[:5] #\\t - 탭"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0UBQQi0DBiF",
        "outputId": "5ce4a638-5635-4a24-ab0d-4e1bf638fff1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['go\\t가', 'hi\\t안녕', 'run\\t뛰어', 'run\\t뛰어', 'who\\t누구']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습용 데이터 만들기"
      ],
      "metadata": {
        "id": "9uJ7SQ2xDNlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 단어가 10개를 넘지 않는 문장들만 사용\n",
        "* 문장을 불러올 때 <EOS(End Of Speech)> 토큰을 추가해서 문장이 끝났음을 알림"
      ],
      "metadata": {
        "id": "8amudxBvDUcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.utils.data.dataset import Dataset"
      ],
      "metadata": {
        "id": "Tp5Ac8oiDn24"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BOW를 만드는 함수 정의"
      ],
      "metadata": {
        "id": "vFh4bwZ8DoOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_BOW(corpus): # 말뭉치 -> 문장 -> BOW를 만드는 함수\n",
        "    BOW = {\"<SOS>\": 0, \"<EOS>\": 1}\n",
        "    # BOW 안에 문장의 시작과 끝을 알리는\n",
        "    # SOS(Start Of Speech) 토큰과 EOS(End Of Speech) 토큰을 추가\n",
        "\n",
        "    # 문장 내 단어들을 사용하여 BOW를 생성\n",
        "    for line in corpus:\n",
        "        for word in line.split():\n",
        "            if word not in BOW.keys(): # 등록되지 않은 단어면\n",
        "                BOW[word] = len(BOW.keys())\n",
        "                # 사전에 추가해주는데, 해당 단어의 고유번호는 이전까지의 키의 갯수\n",
        "    return BOW"
      ],
      "metadata": {
        "id": "WrQ7qjwYD05b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습용 데이터셋 정의"
      ],
      "metadata": {
        "id": "zD_EagauE2fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Eng2Kor(Dataset):\n",
        "    def __init__(self, path='./corpus.txt') -> None:\n",
        "        super().__init__()\n",
        "        self.eng_corpus = [] # 영어문장이 들어가는 변수\n",
        "        self.kor_corpus = [] # 한글문장이 들어가는 변수\n",
        "\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            lines = f.read().split('\\n')\n",
        "            for line in lines: # 문장\n",
        "                txt = \"\".join(v for v in line\n",
        "                              if not v in string.punctuation).lower()\n",
        "                # \\t 구분이 되어 있었음 (영어와 한글) -> 탭을 기준으로 분리\n",
        "                engtxt, kortxt = txt.split('\\t') # 0 : 영어 # 1 : 한글\n",
        "                # engtxt = txt.split('\\t')[0]\n",
        "                # kortxt = txt.split('\\t')[1]\n",
        "\n",
        "                # 길이가 10 이하인 문장 = 단어의 갯수가 10개 이하인 문장만 학습\n",
        "                if len(engtxt.split()) <= 10 and len(kortxt.split()) <= 10:\n",
        "                    # 영어, 한글 번역문 모두 10개 단어 이하인 데이터만 사용\n",
        "                    self.eng_corpus.append(engtxt)\n",
        "                    self.kor_corpus.append(kortxt)\n",
        "        \n",
        "        # 영어와 한글 문장을 각각 BOW(단어 사전)으로 변환\n",
        "        self.engBOW = get_BOW(self.eng_corpus)\n",
        "        self.korBOW = get_BOW(self.kor_corpus)\n",
        "    \n",
        "    # 문장을 단어별로 분리하는 함수\n",
        "    def gen_seq(self, line): # line = 문장\n",
        "        seq = line.split() # 토큰화 한다음에\n",
        "        seq.append(\"<EOS>\") # 마지막에 EOS(문장 끝) 토큰 추가\n",
        "        return seq\n",
        "\n",
        "    def __len__(self): # 데이터의 개수를 반환하는 함수\n",
        "        return len(self.eng_corpus)\n",
        "\n",
        "    # 데이터와 정답을 반환하는 함수\n",
        "    def __getitem__(self, i): # data, label을 지정\n",
        "        # 문자열로 되어 있는 문장을 숫자 표현으로 변경\n",
        "        # 1) 영어 corpus 중 i번째 문장을 받아옴\n",
        "        # 2) gen_seq -> i번째 문장을 seq 형태로 변환 (토큰+EOS)\n",
        "        # 3) 단어 사전을 사용해서 고유번호 형태로 변환 (학습을 위해 숫자형태로 변환)\n",
        "        data = np.array([\n",
        "            self.engBOW[txt] for txt in self.gen_seq(self.eng_corpus[i])\n",
        "        ])\n",
        "        label = np.array([\n",
        "            self.korBOW[txt] for txt in self.gen_seq(self.kor_corpus[i])\n",
        "        ])\n",
        "        return data, label # 영어 데이터 (입력) -> 한글 데이터 (정답)"
      ],
      "metadata": {
        "id": "jFiIQebMEwD_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 로더"
      ],
      "metadata": {
        "id": "7hyu5o55qrQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loader(dataset): # 데이터셋의 문장을 한 문장씩 불러오기 위한 함수 정의\n",
        "    for i in range(len(dataset)):\n",
        "        data, label = dataset[i]\n",
        "\n",
        "        # 데이터와 정답을 반환\n",
        "        yield torch.tensor(data), torch.tensor(label)\n",
        "        # yield : 리턴과 유사, 값을 반복적으로 반환"
      ],
      "metadata": {
        "id": "8_zGEJvFqrBs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 정의"
      ],
      "metadata": {
        "id": "UPINig5DrTOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 인코더 정의\n",
        "* 임베딩층, GRU층"
      ],
      "metadata": {
        "id": "za-92TOSrXSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size) -> None:\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # 임베딩층\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        # GRU층\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        # nn.GRU : GRU 계산. input_size, hidden_size, num_layers)\n",
        "    \n",
        "    def forward(self, x, h): # x: 입력값 / h : 은닉상태\n",
        "        # 배치 차원과 시계열 차원 추가\n",
        "        x = self.embedding(x).view(1, 1, -1)\n",
        "        output, hidden = self.gru(x, h) # output : 문장의 특성, hidden 은닉 상태\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "003BAHh4rehb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 디코더 정의\n",
        "* 임베딩 층\n",
        "* 전결합 층 (ReLU)\n",
        "* 전결합 층 (Softmax)\n",
        "* 내적\n",
        "* GRU층"
      ],
      "metadata": {
        "id": "wNA-b81Itgjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=11) -> None:\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # 임베딩 층 정의\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "\n",
        "        # 어텐션 가중치를 계산하기 위한 MLP층\n",
        "        self.attention = nn.Linear(hidden_size * 2, max_length)\n",
        "        # 10개 + <EOS>(1) = 최대 길이 11개\n",
        "\n",
        "        # 특징 추출을 위한 MLP층\n",
        "        self.context = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "        # 오버피팅을 피하기 위한 드롭아웃층\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # GRU층\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "        # 단어 분류를 위한 MLP층\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # 활성화 함수\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        # LogSoftmax(dim) : 소프트맥스 함수에 로그 값을 취한 것을 반환\n",
        "        # dim -> 계산의 대상이 될 차원값\n",
        "    \n",
        "    def forward(self, x, h, encoder_outputs): # x : 입력값, h : 은닉상태, e...: 인코더 결과값\n",
        "        # 입력 받은 x(현 시점의 디코더 입력)을 임베딩 층을 사용해 밀집 표현으로 변환\n",
        "        # 배치 차원, 시계열 차원, 단어들.\n",
        "        x = self.embedding(x).view(1, 1, -1)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # 어텐션 가중치 계산\n",
        "        attn_weights = self.softmax(\n",
        "            self.attention(torch.cat((x[0], h[0]), -1))\n",
        "        )\n",
        "\n",
        "        # 어텐션 가중치와 인코더의 출력을 내적(크기가 다른 두 배열을 방향이 일치하는 만큼 곱함)\n",
        "        attn_applied = torch.bmm(\n",
        "            attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0)\n",
        "        ) # bmm(A, B) : A 크기가 (B, N, M)이고, B 크기가 (B, M, K)\n",
        "        # => (B, N, K) 크기의 출력을 반환 -> 유사도.\n",
        "        # (얼마나 어텐션 가중치 - 이전까지의 전체 맥락\n",
        "        # -> 인코더 출력 (인코더를 통해서 구한 핵심 내용)) 과의 일치도가 어느정도냐?)\n",
        "\n",
        "        # 인코더 각 시점의 중요도와 밀집 표현을 합쳐서 MLP층으로 특징 추출\n",
        "        output = torch.cat((x[0], attn_applied[0]), 1)\n",
        "        output = self.context(output).unsqueeze(0)\n",
        "        output = self.relu(output)\n",
        "        # 인코더의 중요도(attn_applied)와 현시점에서의 디코더의 밀집표현(x)을 합쳐서\n",
        "        # MLP층(context)으로 입력\n",
        "        # -> MP층은 인코더 각 시점의 중요도와 현시점 디코더의 밀집표현을 동시에 처리\n",
        "        # -> 인코더의 중요도가 디코더의 반영\n",
        "\n",
        "        # GRU층으로 입력\n",
        "        output, hidden = self.gru(output, h)\n",
        "\n",
        "        # 예측된 단어를 출력\n",
        "        output = self.out(output[0])\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "CDqJvnMJtX11"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습 정의"
      ],
      "metadata": {
        "id": "IOuGn4jC44GK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습에 필요한 요소 정의"
      ],
      "metadata": {
        "id": "CyiuJmmk47HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.optim.adam import Adam\n",
        "\n",
        "# 학습에 사용할 프로세서 정의\n",
        "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "# 학습에 사용할 데이터셋\n",
        "dataset = Eng2Kor()\n",
        "\n",
        "# 인코더 디코더 정의\n",
        "encoder = Encoder(input_size=len(dataset.engBOW), hidden_size=64).to(device)\n",
        "decoder = Decoder(64, len(dataset.korBOW), dropout_p=0.1).to(device)\n",
        "# 인코더와 디코더 학습을 위한 최적화 함수 정의\n",
        "encoder_optimizer = Adam(encoder.parameters(), lr=0.001)\n",
        "decoder_optimizer = Adam(decoder.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "42VTmT0Ps72D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qZiVal6U580O",
        "outputId": "6599f899-a2b3-4b23-970f-fd475cab62a8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 루프 정의"
      ],
      "metadata": {
        "id": "lR0tenX96W95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(200):\n",
        "    iterator = tqdm(loader(dataset), total=len(dataset))\n",
        "    total_loss = 0\n",
        "\n",
        "    for data, label in iterator:\n",
        "        data = torch.tensor(data, dtype=torch.long).to(device)\n",
        "        label = torch.tensor(label, dtype=torch.long).to(device)\n",
        "\n",
        "        # 인코더의 초기 은닉 상태\n",
        "        encoder_hidden = torch.zeros(1, 1, 64).to(device)\n",
        "        # 인코더의 모든 시점의 출력을 저장하는 변수\n",
        "        # 최대 단어 10개 + 종료(EOS) -> 11개\n",
        "        encoder_outputs = torch.zeros(11, 64).to(device)\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        # 인코더 동작\n",
        "        for ei in range(len(data)): # data : 토큰화, 고유번호 -> 단어들의 리스트\n",
        "            # ei => data의 인덱스들\n",
        "            # 한 단어씩 인코더에 넣어줌\n",
        "            encoder_output, encoder_hidden = encoder(\n",
        "                data[ei], encoder_hidden)\n",
        "\n",
        "            # 인코더의 은닉상태를 저장\n",
        "            encoder_outputs[ei] = encoder_output[0, 0]\n",
        "            # 1, 1, *64*\n",
        "        \n",
        "        decoder_input = torch.tensor([[0]]).to(device)\n",
        "\n",
        "        # 인코더의 마지막 은닉 상태를 디코더의 초기 은닉 상태로 지정\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        # 디코더 동작\n",
        "        # 티처 포싱 (Teacher Forcing: 교사 강요)\n",
        "        # Seq2Seq 구조에서 현시점의 입력을 (모델의 예측값을 사용하는 대신에) 정답을 이용하는 방법\n",
        "        # 엉뚱한 답을 피하고, 시간 단축을 위해 강제적으로 정답을 넣어주는 기술 (50% 확률로 적용)\n",
        "        use_teacher_forcing = True if random.random() < 0.5 else False\n",
        "        \n",
        "        if use_teacher_forcing:\n",
        "            for di in range(len(label)): # di : data 인덱스\n",
        "                decoder_output = decoder(\n",
        "                    decoder_input, decoder_hidden, encoder_outputs\n",
        "                )\n",
        "\n",
        "                target = torch.tensor(label[di], dtype=torch.long).to(device)                \n",
        "                target = torch.unsqueeze(target, dim=0).to(device)\n",
        "                loss += nn.CrossEntropyLoss()(decoder_output, target)\n",
        "                \n",
        "                # 직접적으로 정답을 다음 시점의 입력으로 넣어줌\n",
        "                decoder_input = target # 바꿔치기\n",
        "        else:\n",
        "            for di in range(len(label)):\n",
        "                decoder_output = decoder(\n",
        "                    decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "                target = torch.tensor(label[di], dtype=torch.long).to(device)                \n",
        "                target = torch.unsqueeze(target, dim=0).to(device)\n",
        "                loss += nn.CrossEntropyLoss()(decoder_output, target)\n",
        "\n",
        "                # 가장 높은 확률을 갖는 단어의 인덱스 topi\n",
        "                topv, topi = decoder_output.topk(1) # top k -> (1)개를 불러옴\n",
        "                \n",
        "                # 디코더의 예측값을 다음 시점의 입력으로 넣어줌\n",
        "                decoder_input = topi.squeeze().detach() # 텐서 -> 값\n",
        "                \n",
        "                if decoder_input.item() == 1: #<EOS> 토큰을 만나면 중지\n",
        "                    break\n",
        "        \n",
        "        # 전체 손실 계산\n",
        "        total_loss += loss.item() / len(dataset)\n",
        "        iterator.set_description(f\"epoch:{epoch+1} loss:{total_loss}\")\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "torch.save(encoder.state_dict(), \"attn_enc.pt\")\n",
        "torch.save(decoder.state_dict(), \"attn_dec.pt\")"
      ],
      "metadata": {
        "id": "oM5eo7gf6UrC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583,
          "referenced_widgets": [
            "1bdf5dcd9f3048f9ad39ffc0e5037334",
            "b09b21be06de43b088679d472e5874ba",
            "5925203a2b3c4c64831be1354a093134",
            "50dd9e47133d4e4c8009f212b74f65b9",
            "addd956e47e84a72822e8f2931092a1e",
            "76f9e796e67546aa8bc3fc4dbd5826e4",
            "021b424dd606405f812d650f0ee64a89",
            "41d06833f2554c7c90154492a552a36c",
            "0faa92443b5a41a4b05f3ff9de93ca6b",
            "f912a336c8a3489dae9eec1b1d0014e5",
            "cdaa4e6616404e8d8916c16741b88074"
          ]
        },
        "outputId": "84a9be5c-4e3a-4b9f-aad7-71c9b7144a3f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3592 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bdf5dcd9f3048f9ad39ffc0e5037334"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-b67003997b3d>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  data = torch.tensor(data, dtype=torch.long).to(device)\n",
            "<ipython-input-13-b67003997b3d>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  label = torch.tensor(label, dtype=torch.long).to(device)\n",
            "<ipython-input-13-b67003997b3d>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  target = torch.tensor(label[di], dtype=torch.long).to(device)\n",
            "<ipython-input-13-b67003997b3d>:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  target = torch.tensor(label[di], dtype=torch.long).to(device)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b67003997b3d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"epoch:{epoch+1} loss:{total_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/BigData23th/Data/raw/main/attn_enc.pt\n",
        "!wget https://github.com/BigData23th/Data/raw/main/attn_dec.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAsM-EwPDLgl",
        "outputId": "1f48bb1c-6136-4753-ec61-7171530a97f8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-07 06:09:27--  https://github.com/BigData23th/Data/raw/main/attn_enc.pt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/BigData23th/Data/main/attn_enc.pt [following]\n",
            "--2023-04-07 06:09:27--  https://raw.githubusercontent.com/BigData23th/Data/main/attn_enc.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 727147 (710K) [application/octet-stream]\n",
            "Saving to: ‘attn_enc.pt’\n",
            "\n",
            "attn_enc.pt         100%[===================>] 710.10K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-04-07 06:09:27 (16.1 MB/s) - ‘attn_enc.pt’ saved [727147/727147]\n",
            "\n",
            "--2023-04-07 06:09:27--  https://github.com/BigData23th/Data/raw/main/attn_dec.pt\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/BigData23th/Data/main/attn_dec.pt [following]\n",
            "--2023-04-07 06:09:28--  https://raw.githubusercontent.com/BigData23th/Data/main/attn_dec.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2857305 (2.7M) [application/octet-stream]\n",
            "Saving to: ‘attn_dec.pt’\n",
            "\n",
            "attn_dec.pt         100%[===================>]   2.72M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-04-07 06:09:28 (41.8 MB/s) - ‘attn_dec.pt’ saved [2857305/2857305]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 성능 평가"
      ],
      "metadata": {
        "id": "Xj6sqB6TDZFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더 가중치 불러오기\n",
        "encoder.load_state_dict(torch.load(\"attn_enc.pt\", map_location=device))\n",
        "decoder.load_state_dict(torch.load(\"attn_dec.pt\", map_location=device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-See_gDDSvg",
        "outputId": "02a0debe-532a-43c0-e873-de9c7c43ea7b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 불러올 영어 문장을 랜덤하게 지정\n",
        "idx = random.randint(0, len(dataset))\n",
        "# 테스트에 사용할 문장\n",
        "input_sentence = dataset.eng_corpus[idx]\n",
        "input_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "axp4313pDnt7",
        "outputId": "cb67198a-1d24-4972-94a3-a15654870aad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i heard that tom attempted suicide'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망이 번역한 문장\n",
        "pred_sentence = \"\""
      ],
      "metadata": {
        "id": "2KR-fniJD-9J"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, label = dataset[idx]\n",
        "data = torch.tensor(data, dtype=torch.long).to(device) # 영어문장\n",
        "label = torch.tensor(label, dtype=torch.long).to(device) # 한국어문장"
      ],
      "metadata": {
        "id": "wpQit4YVEGvY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxAVPvZcEbZl",
        "outputId": "34f9a0a8-339f-488c-c908-3e615c99ffec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 13, 632, 217,  52, 962, 345,   1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr73uVzPEXEp",
        "outputId": "8fa1f703-1391-453d-b733-217b98027b20"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  66, 1576, 3847,  949,    1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 인코더 동작"
      ],
      "metadata": {
        "id": "mTYzESGiEtfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더의 초기 은닉 상태 정의\n",
        "encoder_hidden = torch.zeros(1, 1, 64).to(device)\n",
        "# 인코더 출력을 담기 위한 변수\n",
        "encoder_outputs = torch.zeros(11, 64).to(device)"
      ],
      "metadata": {
        "id": "_WwZLartEu1D"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ei in range(len(data)):\n",
        "    # 한 단어씩 인코더에 넣어줌\n",
        "    encoder_output, encoder_hidden = encoder(\n",
        "        data[ei], encoder_hidden\n",
        "    )\n",
        "    # 인코더의 출력을 저장\n",
        "    encoder_outputs[ei] = encoder_output[0, 0]"
      ],
      "metadata": {
        "id": "13okMEhfE6K5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_outputs"
      ],
      "metadata": {
        "id": "Bumb4Mr1FP8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03297423-92a7-4bc6-94bc-aa3bf057fdd6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0670, -0.0828,  0.1387,  0.4108,  0.3733, -0.4414,  0.0194, -0.2774,\n",
              "         -0.5139, -0.2234, -0.1909, -0.2712, -0.6315, -0.2789, -0.3793, -0.2722,\n",
              "          0.3381,  0.1874,  0.5575, -0.6125, -0.2118,  0.0347,  0.0246,  0.1240,\n",
              "          0.1052, -0.8413,  0.5108,  0.1417,  0.7593, -0.0846,  0.3523, -0.0465,\n",
              "          0.0349,  0.3018,  0.2236,  0.2729, -0.4056, -0.0851, -0.0676,  0.0128,\n",
              "          0.4679, -0.3215,  0.3848, -0.3517, -0.1487,  0.7700,  0.6093, -0.0393,\n",
              "         -0.0622, -0.5013,  0.3958, -0.2262,  0.9282,  0.0950,  0.0999, -0.5390,\n",
              "          0.1563,  0.2714,  0.0749, -0.2940, -0.4514,  0.7106, -0.2316, -0.2117],\n",
              "        [-0.1114,  0.5947,  0.5064, -0.4460,  0.6718,  0.3457,  0.4124, -0.1842,\n",
              "         -0.4232,  0.3502, -0.3818, -0.6414, -0.0434,  0.6135,  0.3980, -0.4125,\n",
              "         -0.2556,  0.7789, -0.8594, -0.8947,  0.2525,  0.1198,  0.1293, -0.4288,\n",
              "         -0.5855, -0.9167, -0.6086,  0.2761,  0.9689,  0.6857,  0.4791, -0.3535,\n",
              "          0.6670, -0.6891,  0.6454,  0.0958,  0.6857, -0.3823, -0.4769,  0.0331,\n",
              "         -0.7400, -0.3323,  0.6026, -0.0309,  0.3479,  0.6462, -0.7179,  0.2729,\n",
              "         -0.0178, -0.2355, -0.2879, -0.3771,  0.1325, -0.1102, -0.3130, -0.8126,\n",
              "         -0.1348, -0.0845,  0.2989, -0.4064, -0.4170, -0.1369, -0.2961, -0.4267],\n",
              "        [-0.1427, -0.2474,  0.5747, -0.8167,  0.7055,  0.3175,  0.1825, -0.2165,\n",
              "         -0.7265, -0.2221, -0.5436,  0.0903, -0.1191, -0.9763,  0.2620, -0.1895,\n",
              "          0.1260,  0.7312, -0.5510,  0.2411,  0.1261,  0.4162,  0.6833, -0.4853,\n",
              "         -0.8149, -0.4429, -0.5358,  0.3264, -0.1558,  0.8040,  0.0129,  0.2269,\n",
              "          0.6921,  0.0141,  0.3063,  0.4679,  0.8036,  0.8557, -0.6392, -0.3982,\n",
              "         -0.8203,  0.2502, -0.3764,  0.0802, -0.7760, -0.1737, -0.2263,  0.2741,\n",
              "         -0.0953, -0.4175, -0.2211, -0.3500,  0.1470, -0.1800, -0.5419, -0.9051,\n",
              "         -0.3859, -0.1970,  0.7163, -0.5754, -0.3860, -0.7053, -0.5282, -0.7614],\n",
              "        [ 0.3574,  0.0915, -0.6215,  0.8547,  0.7578, -0.5190, -0.6938, -0.4035,\n",
              "         -0.2519, -0.8930, -0.7448,  0.8148, -0.0478, -0.9908,  0.0418, -0.4999,\n",
              "          0.7722,  0.8255,  0.1075,  0.5163, -0.8200,  0.1127,  0.4474, -0.2088,\n",
              "          0.3334,  0.7313, -0.0829, -0.8486, -0.8464,  0.1052,  0.7281,  0.2949,\n",
              "         -0.5625,  0.6892,  0.5847,  0.5804,  0.7542,  0.7854, -0.6935, -0.4369,\n",
              "         -0.8357,  0.4478, -0.4970,  0.8022, -0.9528, -0.4466,  0.5727,  0.5524,\n",
              "         -0.4216, -0.5812, -0.0470,  0.2697,  0.7319,  0.2392,  0.3777, -0.9750,\n",
              "         -0.1480, -0.1129, -0.3652,  0.1909, -0.5136, -0.8969,  0.4329, -0.8068],\n",
              "        [ 0.7826, -0.8213, -0.3564,  0.5718,  0.8623,  0.5693, -0.4257,  0.0213,\n",
              "         -0.5184, -0.8452, -0.5185,  0.3518,  0.0436,  0.9946, -0.1602, -0.2994,\n",
              "          0.9234,  0.3630,  0.9558,  0.5966, -0.8773, -0.8602,  0.4828,  0.6842,\n",
              "         -0.4449,  0.8446, -0.6978, -0.6959, -0.7682, -0.5498, -0.1455,  0.3924,\n",
              "          0.0010,  0.6331,  0.5092, -0.0281,  0.7056,  0.8506, -0.8181,  0.7169,\n",
              "         -0.4988,  0.4078,  0.2320,  0.8106, -0.5504, -0.8039,  0.3881, -0.6097,\n",
              "         -0.0532, -0.3494, -0.2148,  0.6335,  0.3853,  0.0576,  0.5603, -0.8846,\n",
              "         -0.5653, -0.1743, -0.7429,  0.6408, -0.5737, -0.9076,  0.9330, -0.7197],\n",
              "        [-0.6197, -0.3872, -0.2280,  0.2251,  0.6338,  0.4437, -0.5280,  0.4534,\n",
              "         -0.9166, -0.5128,  0.7797, -0.9614,  0.1428, -0.9748, -0.4356,  0.5348,\n",
              "          0.7918,  0.7153,  0.9619,  0.5332,  0.5536, -0.5484, -0.1875,  0.8303,\n",
              "         -0.5983, -0.9091, -0.8787, -0.8624,  0.5963,  0.0496,  0.0133,  0.4652,\n",
              "          0.3899,  0.2018,  0.8176, -0.0890,  0.4711, -0.2751, -0.3294,  0.6948,\n",
              "         -0.8194,  0.4757,  0.1079,  0.6566, -0.2415, -0.8697,  0.3928,  0.1538,\n",
              "          0.6218, -0.8399, -0.5833, -0.0011,  0.9163, -0.3570,  0.6685, -0.7115,\n",
              "         -0.3660,  0.4529, -0.0736,  0.2051,  0.6748, -0.9148,  0.8494, -0.3205],\n",
              "        [-0.9984,  0.9972, -0.9938, -0.3160, -0.9964,  0.0451, -0.9889,  0.3673,\n",
              "         -0.9168,  0.8600, -0.9973,  0.9995,  0.8272, -0.9748,  0.1450, -0.9949,\n",
              "         -0.8088,  0.9995,  0.9619,  0.9996,  0.9996,  0.7782, -0.3990,  0.9998,\n",
              "          0.9988,  0.9988, -0.8166,  0.9943, -0.9326,  0.0496,  0.0678, -0.6544,\n",
              "          0.2074,  0.3333,  0.9464,  0.7043,  0.4707, -0.8430,  0.9454,  0.4240,\n",
              "         -0.8015,  0.9973, -0.0191,  0.9580,  0.9998, -0.9992,  0.9909,  0.9997,\n",
              "          0.1145, -1.0000,  0.8088,  0.0110, -0.9990, -0.9998,  0.9990,  0.9994,\n",
              "         -0.8512,  0.9929, -0.0888,  0.9997,  0.9998, -0.8164,  0.6610,  0.7059],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
              "       device='cuda:0', grad_fn=<CopySlices>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 디코더 동작"
      ],
      "metadata": {
        "id": "4JsCvUSrFMaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더 초기 입력\n",
        "decoder_input = torch.tensor([[0]]).to(device)\n",
        "# 0 -> 문장이 시작되었다는 SOS 토큰\n",
        "\n",
        "# 인코더의 마지막 은닉 상태 -> 디코더의 초기 은닉 상태\n",
        "decoder_hidden = encoder_hidden"
      ],
      "metadata": {
        "id": "YtbFVngTFN5r"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for di in range(11):\n",
        "    # 디코더 모델을 통해서 단어별 나올 확률\n",
        "    decoder_output = decoder(\n",
        "        decoder_input, decoder_hidden, encoder_outputs\n",
        "    )\n",
        "    # 가장 높은 확률을 갖는 단어의 요소 계산\n",
        "    topv, topi = decoder_output.topk(1)\n",
        "    # 가장 높은 확률의 단어\n",
        "    decoder_input = topi.squeeze().detach()\n",
        "\n",
        "    # EOS 토큰을 만나면 중지\n",
        "    if decoder_input.item() == 1:\n",
        "        break\n",
        "    \n",
        "    # 예측 문자열에 가장 높은 확률의 단어를 추가\n",
        "    pred_sentence += list(dataset.korBOW.keys())[decoder_input] + \" \"\n",
        "\n",
        "print(input_sentence)\n",
        "print(pred_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRyfeY8OFjuB",
        "outputId": "c1d62e1b-67f8-4e39-a7fd-123f79e737df"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i heard that tom attempted suicide\n",
            "톰이 시도했다고 들었어 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 통합"
      ],
      "metadata": {
        "id": "uQdzZcLYJi11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gtts -q"
      ],
      "metadata": {
        "id": "5NyU4tGQKhrH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.optim.adam import Adam\n",
        "\n",
        "# 학습에 사용할 프로세서 정의\n",
        "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "# 학습에 사용할 데이터셋\n",
        "dataset = Eng2Kor()\n",
        "\n",
        "# 인코더 디코더 정의\n",
        "encoder = Encoder(input_size=len(dataset.engBOW), hidden_size=64).to(device)\n",
        "decoder = Decoder(64, len(dataset.korBOW), dropout_p=0.1).to(device)\n",
        "\n",
        "# 인코더 가중치 불러오기\n",
        "encoder.load_state_dict(torch.load(\"attn_enc.pt\", map_location=device))\n",
        "# 디코더 가중치 불러오기\n",
        "decoder.load_state_dict(torch.load(\"attn_dec.pt\", map_location=device))\n",
        "\n",
        "idx = random.randint(0, len(dataset))\n",
        "# 테스트에 사용할 문장\n",
        "input_sentence = dataset.eng_corpus[idx]\n",
        "# 신경망이 번역한 문장\n",
        "pred_sentence = \"\"\n",
        "\n",
        "data, label = dataset[idx]\n",
        "data = torch.tensor(data, dtype=torch.long).to(device)\n",
        "label = torch.tensor(label, dtype=torch.long).to(device)\n",
        "\n",
        "encoder_hidden = torch.zeros(1, 1, 64).to(device)\n",
        "encoder_outputs = torch.zeros(11, 64).to(device)\n",
        "\n",
        "for ei in range(len(data)):\n",
        "   encoder_output, encoder_hidden = encoder(\n",
        "       data[ei], encoder_hidden)\n",
        "     \n",
        "   encoder_outputs[ei] = encoder_output[0, 0]  \n",
        "\n",
        "decoder_input = torch.tensor([[0]]).to(device)\n",
        "\n",
        "decoder_hidden = encoder_hidden \n",
        "\n",
        "for di in range(11):\n",
        "   decoder_output = decoder(\n",
        "                       decoder_input, decoder_hidden, encoder_outputs)\n",
        "   topv, topi = decoder_output.topk(1)\n",
        "   decoder_input = topi.squeeze().detach()\n",
        "\n",
        "   if decoder_input.item() == 1:  \n",
        "       break\n",
        "\n",
        "   pred_sentence += list(dataset.korBOW.keys())[decoder_input] + \" \"  \n",
        "\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio\n",
        "from time import sleep\n",
        "\n",
        "file_name = '/content/sample.mp3'\n",
        "\n",
        "text = input_sentence\n",
        "tts_en = gTTS(text=text)\n",
        "tts_en.save(file_name)\n",
        "\n",
        "print(input_sentence)  # 영어 문장\n",
        "wn = Audio(file_name, autoplay=True)\n",
        "display(wn)\n",
        "\n",
        "sleep(4)\n",
        "\n",
        "text = pred_sentence\n",
        "tts_ko = gTTS(text=text, lang='ko')\n",
        "tts_ko.save(file_name)\n",
        "print(pred_sentence)  # 한글 문장\n",
        "\n",
        "wn = Audio(file_name, autoplay=True)\n",
        "display(wn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "OmQ0IotPJkF_",
        "outputId": "78d22580-f892-4cdf-d6c1-2f63e4e7f26a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we remember\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
              "                    <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "우린 기억하고 있어 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
              "                    <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}